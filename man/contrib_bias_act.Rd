% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/contrib.R
\name{contrib_bias_act}
\alias{contrib_bias_act}
\title{Fused bias and activation function.}
\usage{
contrib_bias_act(
  x,
  b = NULL,
  dim = 1,
  act = "linear",
  alpha = NULL,
  gain = NULL,
  clamp = NULL,
  impl = if (cuda_is_available() & x$device$type == "cuda") "cuda" else "ref"
)
}
\arguments{
\item{x}{Input activation tensor. Can be of any shape.}

\item{b}{Bias vector, or \code{NULL} to disable. Must be a 1D tensor of the same type
as \code{x}. The shape must be known, and it must match the dimension of \code{x}
corresponding to \code{dim}.}

\item{dim}{The dimension in \code{x} corresponding to the elements of \code{b}.
The value of \code{dim} is ignored if \code{b} is not specified.}

\item{act}{Name of the activation function to evaluate, or \code{"linear"} to disable.
Can be e.g. \code{"relu"}, \code{"lrelu"}, \code{"tanh"}, \code{"sigmoid"}, \code{"swish"}, etc.
See details for a full list. \code{NULL} is not allowed.}

\item{alpha}{Shape parameter for the activation function, or \code{NULL} to use the default.}

\item{gain}{Scaling factor for the output tensor, or \code{NULL} to use default.
See details for the default scaling of each activation function.
If unsure, consider specifying 1.}

\item{clamp}{Clamp the output values to \code{c(-clamp, +clamp)}, or \code{NULL} to disable
the clamping (default).}

\item{impl}{Name of the implementation to use. Can be \code{"ref"} or \code{"cuda"}.}
}
\value{
\code{torch_tensor} of the same shape and datatype as \code{x}.
}
\description{
Adds bias \code{b} to activation tensor \code{x}, evaluates activation function \code{act},
and scales the result by \code{gain}. Each of the steps is optional. In most cases,
the fused op is considerably more efficient than performing the same calculation
using standard PyTorch ops. It supports first and second order gradients,
but not third order gradients.
}
\section{Activation Functions}{
NA
}

