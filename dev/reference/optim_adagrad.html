<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Adagrad optimizer — optim_adagrad • torch</title><!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/united/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous"><script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css"><script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous"><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet"><script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet"><meta property="og:title" content="Adagrad optimizer — optim_adagrad"><meta property="og:description" content="Proposed in Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"><meta name="robots" content="noindex"><!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-178883486-1"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178883486-1');
</script></head><body data-spy="scroll" data-target="#toc">
    

    <div class="container template-reference-topic">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">torch</a>
        <span class="version label label-danger" data-toggle="tooltip" data-placement="bottom" title="In-development version">0.6.1.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav"><li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu"><li>
      <a href="../articles/installation.html">Installation</a>
    </li>
    <li class="dropdown-header">Tensors</li>
    <li>
      <a href="../articles/tensor-creation.html">Creating tensors</a>
    </li>
    <li>
      <a href="../articles/indexing.html">Indexing</a>
    </li>
    <li>
      <a href="../articles/tensor/index.html">Tensor class</a>
    </li>
    <li>
      <a href="../articles/serialization.html">Serialization</a>
    </li>
    <li class="dropdown-header">Datasets</li>
    <li>
      <a href="../articles/loading-data.html">Loading Data</a>
    </li>
    <li class="dropdown-header">Autograd</li>
    <li>
      <a href="../articles/using-autograd.html">Using autograd</a>
    </li>
    <li>
      <a href="../articles/extending-autograd.html">Extending autograd</a>
    </li>
    <li>
      <a href="../articles/python-to-r.html">Python models</a>
    </li>
  </ul></li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Examples
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu"><li>
      <a href="../articles/examples/basic-autograd.html">basic-autograd</a>
    </li>
    <li>
      <a href="../articles/examples/basic-nn-module.html">basic-nn-module</a>
    </li>
    <li>
      <a href="../articles/examples/dataset.html">dataset</a>
    </li>
  </ul></li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul><ul class="nav navbar-nav navbar-right"><li>
  <a href="https://github.com/mlverse/torch/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul></div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Adagrad optimizer</h1>
    <small class="dont-index">Source: <a href="https://github.com/mlverse/torch/blob/HEAD/R/optim-adagrad.R" class="external-link"><code>R/optim-adagrad.R</code></a></small>
    <div class="hidden name"><code>optim_adagrad.Rd</code></div>
    </div>

    <div class="ref-description">
    <p>Proposed in <a href="https://jmlr.org/papers/v12/duchi11a.html" class="external-link">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a></p>
    </div>

    <div id="ref-usage">
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="fu">optim_adagrad</span><span class="op">(</span>
  <span class="va">params</span>,
  lr <span class="op">=</span> <span class="fl">0.01</span>,
  lr_decay <span class="op">=</span> <span class="fl">0</span>,
  weight_decay <span class="op">=</span> <span class="fl">0</span>,
  initial_accumulator_value <span class="op">=</span> <span class="fl">0</span>,
  eps <span class="op">=</span> <span class="fl">1e-10</span>
<span class="op">)</span></code></pre></div>
    </div>

    <div id="arguments">
    <h2>Arguments</h2>
    <dl><dt>params</dt>
<dd><p>(iterable): list of parameters to optimize or list parameter groups</p></dd>
<dt>lr</dt>
<dd><p>(float, optional): learning rate (default: 1e-2)</p></dd>
<dt>lr_decay</dt>
<dd><p>(float, optional): learning rate decay (default: 0)</p></dd>
<dt>weight_decay</dt>
<dd><p>(float, optional): weight decay (L2 penalty) (default: 0)</p></dd>
<dt>initial_accumulator_value</dt>
<dd><p>the initial value for the accumulator. (default: 0)</p>
<p>Adagrad is an especially good optimizer for sparse data.
It individually modifies learning rate for every single parameter,
dividing the original learning rate value by sum of the squares of the gradients.
It causes that the rarely occurring features get greater learning rates.
The main downside of this method is the fact that learning rate may be
getting small too fast, so that at some point a model cannot learn anymore.</p></dd>
<dt>eps</dt>
<dd><p>(float, optional): term added to the denominator to improve
numerical stability (default: 1e-10)</p></dd>
</dl></div>
    <div id="note">
    <h2>Note</h2>
    <p>Update rule:
$$
\theta_{t+1} = \theta_{t} - \frac{\eta }{\sqrt{G_{t} + \epsilon}} \odot g_{t}
$$
The equation above and some remarks quoted
after <a href="https://ruder.io/optimizing-gradient-descent/index.html#adagrad" class="external-link"><em>An overview of gradient descent optimization algorithms</em></a>
by Sebastian Ruder.</p>
    </div>
    <div id="warning">
    <h2>Warning</h2>
    <p>If you need to move a model to GPU via <code>$cuda()</code>, please do so before
constructing optimizers for it. Parameters of a model after <code>$cuda()</code>
will be different objects from those before the call. In general, you
should make sure that the objects pointed to by model parameters subject
to optimization remain the same over the whole lifecycle of optimizer
creation and usage.</p>
    </div>

  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <nav id="toc" data-toggle="toc" class="sticky-top"><h2 data-toc-skip>Contents</h2>
    </nav></div>
</div>


      <footer><div class="copyright">
  <p></p><p>Developed by Daniel Falbel, Javier Luraschi.</p>
</div>

<div class="pkgdown">
  <p></p><p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.2.</p>
</div>

      </footer></div>

  


  

  </body></html>

